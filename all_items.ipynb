{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyzotero import zotero\n",
    "import os\n",
    "# import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json, sys\n",
    "from datetime import date, timedelta  \n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import pycountry\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"# All items in the Zotero Intelligence bibliography library\".index(\"All\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_id = '2514686'\n",
    "library_type = 'group'\n",
    "api_key = '' # api_key is only needed for private groups and libraries\n",
    "zot = zotero.Zotero(library_id, library_type)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All items in the Zotero Intelligence bibliography library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = zot.everything(zot.top())\n",
    "\n",
    "data3=[]\n",
    "columns3=['Title','Publication type', 'Link to publication', 'Abstract', 'Zotero link', 'Date published', 'FirstName2', 'Publisher', 'Journal', 'Date added', 'Col key', 'DOI', 'Book_title', 'Thesis_type', 'University']\n",
    "\n",
    "for item in items:\n",
    "    # Extracting various types of creators\n",
    "    authors = \"\"\n",
    "    if 'creators' in item['data']:\n",
    "        authors_info = item['data']['creators']\n",
    "        author_names = []\n",
    "        for author in authors_info:\n",
    "            creator_type = author.get('creatorType')\n",
    "            valid_types = ['author', 'contributor', 'editor', 'guest', 'podcaster', 'presenter', 'translator', 'programmer']\n",
    "            \n",
    "            if item['data']['itemType'] == 'bookSection' and creator_type == 'editor':\n",
    "                continue  # Skip adding 'editor' for Book chapters\n",
    "            \n",
    "            if creator_type in valid_types:\n",
    "                if 'firstName' in author and 'lastName' in author:\n",
    "                    author_names.append(author['firstName'] + ' ' + author['lastName'])\n",
    "                elif 'name' in author:\n",
    "                    author_names.append(author['name'])\n",
    "                \n",
    "        authors = ', '.join(author_names)\n",
    "        \n",
    "    data3.append((\n",
    "        item['data']['title'], \n",
    "        item['data']['itemType'], \n",
    "        item['data']['url'], \n",
    "        item['data']['abstractNote'], \n",
    "        item['links']['alternate']['href'],\n",
    "        item['data'].get('date'),\n",
    "        authors,  # Add the concatenated authors string\n",
    "        item['data'].get('publisher'),\n",
    "        item['data'].get('publicationTitle'),\n",
    "        item['data']['dateAdded'],\n",
    "        item['data']['collections'],\n",
    "        item['data'].get('DOI'),\n",
    "        item['data'].get('bookTitle'),\n",
    "        item['data'].get('thesisType', ''),\n",
    "        item['data'].get('university', '')\n",
    "        ))\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame(data3, columns=columns3)\n",
    "\n",
    "mapping_types = {\n",
    "    'thesis': 'Thesis',\n",
    "    'journalArticle': 'Journal article',\n",
    "    'book': 'Book',\n",
    "    'bookSection': 'Book chapter',\n",
    "    'blogPost': 'Blog post',\n",
    "    'videoRecording': 'Video',\n",
    "    'podcast': 'Podcast',\n",
    "    'magazineArticle': 'Magazine article',\n",
    "    'webpage': 'Webpage',\n",
    "    'newspaperArticle': 'Newspaper article',\n",
    "    'report': 'Report',\n",
    "    'forumPost': 'Forum post',\n",
    "    'manuscript': 'Manuscript',\n",
    "    'document': 'Document',\n",
    "    'conferencePaper': 'Conference paper',\n",
    "    'film': 'Film',\n",
    "    'presentation': 'Presentation',\n",
    "    'audioRecording':'Podcast',\n",
    "    'preprint':'Preprint',\n",
    "    'hearing':'Hearing',\n",
    "    'computerProgram':'Computer program',\n",
    "    'dataset':'Dataset'\n",
    "}\n",
    "df['Publication type'] = df['Publication type'].replace(mapping_types)\n",
    "\n",
    "mapping_publisher = {\n",
    "    'Taylor & Francis Group': 'Taylor and Francis',\n",
    "    'Taylor and Francis': 'Taylor and Francis',\n",
    "    'Taylor & Francis': 'Taylor and Francis',\n",
    "    'Routledge': 'Routledge',\n",
    "    'Routledge Handbooks Online': 'Routledge',\n",
    "    'Praeger Security International': 'Praeger',\n",
    "    'Praeger': 'Praeger'\n",
    "}\n",
    "df['Publisher'] = df['Publisher'].replace(mapping_publisher)\n",
    "\n",
    "# df['Publisher'] = df['Publisher'].replace(['Taylor & Francis Group', 'Taylor and Francis', 'Taylor & Francis'], 'Taylor and Francis')\n",
    "# df['Publisher'] = df['Publisher'].replace(['Routledge', 'Routledge Handbooks Online'], 'Routledge')\n",
    "# df['Publisher'] = df['Publisher'].replace(['Praeger Security International', 'Praeger'], 'Praeger')\n",
    "\n",
    "mapping_journal = {\n",
    "    'International Journal of Intelligence and Counterintelligence': 'Intl Journal of Intelligence and Counterintelligence',\n",
    "    'International Journal of Intelligence and CounterIntelligence': 'Intl Journal of Intelligence and Counterintelligence',\n",
    "    'International Journal of Intelligence and Counter Intelligence': 'Intl Journal of Intelligence and Counterintelligence',\n",
    "    'Intelligence and national security': 'Intelligence and National Security',\n",
    "    'Intelligence and National Security': 'Intelligence and National Security',\n",
    "    'Intelligence & National Security': 'Intelligence and National Security'\n",
    "}\n",
    "\n",
    "df['Journal'] = df['Journal'].replace(mapping_journal)\n",
    "\n",
    "mapping_thesis_type ={\n",
    "    \"MA Thesis\": \"Master's Thesis\",\n",
    "    \"PhD Thesis\": \"PhD Thesis\",\n",
    "    \"Master Thesis\": \"Master's Thesis\",\n",
    "    \"Thesis\": \"Master's Thesis\",  # Assuming 'Thesis' refers to Master's Thesis here, adjust if necessary\n",
    "    \"Ph.D.\": \"PhD Thesis\",\n",
    "    \"Master's Dissertation\": \"Master's Thesis\",\n",
    "    \"Undergraduate Theses\": \"Undergraduate Thesis\",\n",
    "    \"MPhil\": \"MPhil Thesis\",\n",
    "    \"A.L.M.\": \"Master's Thesis\",  # Assuming A.L.M. (Master of Liberal Arts) maps to Master's Thesis\n",
    "    \"doctoralThesis\": \"PhD Thesis\",\n",
    "    \"PhD\": \"PhD Thesis\",\n",
    "    \"Masters\": \"Master's Thesis\",\n",
    "    \"PhD thesis\": \"PhD Thesis\",\n",
    "    \"phd\": \"PhD Thesis\",\n",
    "    \"doctoral\": \"PhD Thesis\",\n",
    "    \"Doctoral\": \"PhD Thesis\",\n",
    "    \"Master of Arts Dissertation\": \"Master's Thesis\",\n",
    "    \"\":'Unclassified'\n",
    "}\n",
    "df['Thesis_type'] = df['Thesis_type'].replace(mapping_thesis_type)\n",
    "# df['Journal'] = df['Journal'].replace(['International Journal of Intelligence and Counterintelligence', 'International Journal of Intelligence and CounterIntelligence'], 'Intl Journal of Intelligence and Counterintelligence')\n",
    "# df['Journal'] = df['Journal'].replace(['Intelligence and national security', 'Intelligence and National Security', 'Intelligence & National Security'], 'Intelligence and National Security')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df[df['Title']=='Sir Robert Cecil and Elizabethan Intelligencing, 1590-1603']\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items = zot.everything(zot.top())\n",
    "\n",
    "# data3=[]\n",
    "# columns3=['Title','Publication type', 'Link to publication', 'Abstract', 'Zotero link', 'Date published', 'FirstName2', 'Publisher', 'Journal', 'Date added', 'Col key']\n",
    "\n",
    "# for item in items:\n",
    "#     # Extracting various types of creators\n",
    "#     authors = \"\"\n",
    "#     if 'creators' in item['data']:\n",
    "#         authors_info = item['data']['creators']\n",
    "#         author_names = []\n",
    "#         for author in authors_info:\n",
    "#             creator_type = author.get('creatorType')\n",
    "#             valid_types = ['author', 'contributor', 'editor', 'guest', 'podcaster']\n",
    "#             if creator_type in valid_types:  # Include all identified creator types\n",
    "#                 if 'firstName' in author and 'lastName' in author:\n",
    "#                     author_names.append(author['firstName'] + ' ' + author['lastName'])\n",
    "#                 elif 'name' in author:\n",
    "#                     author_names.append(author['name'])  # Handling cases where 'firstName' and 'lastName' are absent\n",
    "                \n",
    "#         authors = ', '.join(author_names)\n",
    "        \n",
    "#     data3.append((\n",
    "#         item['data']['title'], \n",
    "#         item['data']['itemType'], \n",
    "#         item['data']['url'], \n",
    "#         item['data']['abstractNote'], \n",
    "#         item['links']['alternate']['href'],\n",
    "#         item['data'].get('date'),\n",
    "#         authors,  # Add the concatenated authors string\n",
    "#         item['data'].get('publisher'),\n",
    "#         item['data'].get('publicationTitle'),\n",
    "#         item['data']['dateAdded'],\n",
    "#         item['data']['collections']\n",
    "#         )) \n",
    "# # pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# df = pd.DataFrame(data3, columns=columns3)\n",
    "\n",
    "# mapping_types = {\n",
    "#     'thesis': 'Thesis',\n",
    "#     'journalArticle': 'Journal article',\n",
    "#     'book': 'Book',\n",
    "#     'bookSection': 'Book chapter',\n",
    "#     'blogPost': 'Blog post',\n",
    "#     'videoRecording': 'Video',\n",
    "#     'podcast': 'Podcast',\n",
    "#     'magazineArticle': 'Magazine article',\n",
    "#     'webpage': 'Webpage',\n",
    "#     'newspaperArticle': 'Newspaper article',\n",
    "#     'report': 'Report',\n",
    "#     'forumPost': 'Forum post',\n",
    "#     'manuscript': 'Manuscript',\n",
    "#     'document': 'Document',\n",
    "#     'conferencePaper': 'Conference paper',\n",
    "#     'film': 'Film',\n",
    "#     'presentation': 'Presentation',\n",
    "#     'audioRecording':'Podcast'\n",
    "# }\n",
    "# df['Publication type'] = df['Publication type'].replace(mapping_types)\n",
    "\n",
    "# mapping_publisher = {\n",
    "#     'Taylor & Francis Group': 'Taylor and Francis',\n",
    "#     'Taylor and Francis': 'Taylor and Francis',\n",
    "#     'Taylor & Francis': 'Taylor and Francis',\n",
    "#     'Routledge': 'Routledge',\n",
    "#     'Routledge Handbooks Online': 'Routledge',\n",
    "#     'Praeger Security International': 'Praeger',\n",
    "#     'Praeger': 'Praeger'\n",
    "# }\n",
    "# df['Publisher'] = df['Publisher'].replace(mapping_publisher)\n",
    "\n",
    "# # df['Publisher'] = df['Publisher'].replace(['Taylor & Francis Group', 'Taylor and Francis', 'Taylor & Francis'], 'Taylor and Francis')\n",
    "# # df['Publisher'] = df['Publisher'].replace(['Routledge', 'Routledge Handbooks Online'], 'Routledge')\n",
    "# # df['Publisher'] = df['Publisher'].replace(['Praeger Security International', 'Praeger'], 'Praeger')\n",
    "\n",
    "# mapping_journal = {\n",
    "#     'International Journal of Intelligence and Counterintelligence': 'Intl Journal of Intelligence and Counterintelligence',\n",
    "#     'International Journal of Intelligence and CounterIntelligence': 'Intl Journal of Intelligence and Counterintelligence',\n",
    "#     'Intelligence and national security': 'Intelligence and National Security',\n",
    "#     'Intelligence and National Security': 'Intelligence and National Security',\n",
    "#     'Intelligence & National Security': 'Intelligence and National Security'\n",
    "# }\n",
    "\n",
    "# df['Journal'] = df['Journal'].replace(mapping_journal)\n",
    "\n",
    "# # df['Journal'] = df['Journal'].replace(['International Journal of Intelligence and Counterintelligence', 'International Journal of Intelligence and CounterIntelligence'], 'Intl Journal of Intelligence and Counterintelligence')\n",
    "# # df['Journal'] = df['Journal'].replace(['Intelligence and national security', 'Intelligence and National Security', 'Intelligence & National Security'], 'Intelligence and National Security')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zotero_collections2(library_id, library_type):\n",
    "    collections = zot.collections()\n",
    "    data = [(item['data']['key'], item['data']['name'], item['meta']['numItems'], item['links']['alternate']['href']) for item in collections]\n",
    "    df_collections = pd.DataFrame(data, columns=['Key', 'Name', 'Number', 'Link'])\n",
    "    return df_collections\n",
    "df_collections_2 = zotero_collections2(library_id, library_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows_by_col_key(df, df_collections):\n",
    "    # Duplicate rows based on 'Col key'\n",
    "    duplicated_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if isinstance(row['Col key'], list):\n",
    "            for key in row['Col key']:\n",
    "                new_row = row.copy()\n",
    "                collection_info = df_collections[df_collections['Key'] == key]\n",
    "                if not collection_info.empty:\n",
    "                    new_row['Collection_Key'] = key\n",
    "                    new_row['Collection_Name'] = collection_info.iloc[0]['Name']\n",
    "                    new_row['Collection_Link'] = collection_info.iloc[0]['Link']\n",
    "                    duplicated_rows.append(new_row)\n",
    "        else:\n",
    "            key = row['Col key']\n",
    "            new_row = row.copy()\n",
    "            collection_info = df_collections[df_collections['Key'] == key]\n",
    "            if not collection_info.empty:\n",
    "                new_row['Collection_Key'] = key\n",
    "                new_row['Collection_Name'] = collection_info.iloc[0]['Name']\n",
    "                new_row['Collection_Link'] = collection_info.iloc[0]['Link']\n",
    "                duplicated_rows.append(new_row)\n",
    "\n",
    "    # Create a new DataFrame with duplicated rows\n",
    "    duplicated_df = pd.DataFrame(duplicated_rows)\n",
    "\n",
    "    return duplicated_df\n",
    "\n",
    "# Duplicating rows based on 'Col key' and collections information\n",
    "duplicated_df = duplicate_rows_by_col_key(df, df_collections_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day_allitems = datetime.date.today().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fa = df['FirstName2']\n",
    "# df_fa = pd.DataFrame(df_fa.tolist())\n",
    "# df_fa = df_fa[0]\n",
    "# df_fa = df_fa.apply(lambda x: {} if pd.isna(x) else x)\n",
    "# df_new = pd.json_normalize(df_fa, errors='ignore') \n",
    "# df = pd.concat([df, df_new], axis=1)\n",
    "# df['firstName'] = df['firstName'].fillna('null')\n",
    "# df['lastName'] = df['lastName'].fillna('null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_items.csv')\n",
    "duplicated_df.to_csv('all_items_duplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import psycopg2\n",
    "# from sqlalchemy import create_engine\n",
    "# import psycopg2.extras as extras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv file into a pandas dataframe\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/YusufAliOzkan/zotero-intelligence-bibliography/main/all_items.csv')\n",
    "\n",
    "# Dictionary to map non-proper country names to their proper names\n",
    "country_map = {\n",
    "    'british': 'UK',\n",
    "    'great britain': 'UK',\n",
    "    'UK' : 'UK', \n",
    "    'america' : 'United States',\n",
    "    'United States of America' : 'United States',\n",
    "    'Soviet Union': 'Russia', \n",
    "    'american' : 'United States',\n",
    "    'United States' : 'United States',\n",
    "    'russian' : 'Russia'\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Find the country names in the \"title\" column of the dataframe\n",
    "found_countries = {}\n",
    "for i, row in df.iterrows():\n",
    "    title = str(row['Title']).lower()\n",
    "    for country in pycountry.countries:\n",
    "        name = country.name.lower()\n",
    "        if name in title or (name + 's') in title:  # Check for singular and plural forms of country names\n",
    "            proper_name = country.name\n",
    "            found_countries[proper_name] = found_countries.get(proper_name, 0) + 1\n",
    "    for non_proper, proper in country_map.items():\n",
    "        if non_proper in title:\n",
    "            found_countries[proper] = found_countries.get(proper, 0) + title.count(non_proper)\n",
    "\n",
    "# Create a new dataframe containing the found countries and their counts\n",
    "df_countries = pd.DataFrame({'Country': list(found_countries.keys()), 'Count': list(found_countries.values())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(df_countries, locations='Country', locationmode='country names', color='Count', \n",
    "                    title='Country mentions in titles', color_continuous_scale='Viridis',\n",
    "                    width=1100, height=700) # Adjust the size of the map here\n",
    "\n",
    "# Display the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries=df_countries.sort_values(by='Count', ascending=False)\n",
    "df_countries.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(df_countries, locations='Country', locationmode='country names', color='Count', \n",
    "                    title='Country mentions in titles', color_continuous_scale='Viridis',\n",
    "                    width=1100, height=700) # Adjust the size of the map here\n",
    "\n",
    "# Display the map\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_countries.to_csv('countries.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import ast\n",
    "from spacy.pipeline import EntityRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/YusufAliOzkan/zotero-intelligence-bibliography/main/all_items.csv')\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    # Download the model if not present\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MI6\"}]\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    orgs = []\n",
    "    gpes = []\n",
    "    people = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == 'ORG':\n",
    "            orgs.append(entity.text)\n",
    "        elif entity.label_ == 'GPE':\n",
    "            gpes.append(entity.text)\n",
    "        elif entity.label_ == 'PERSON':\n",
    "            people.append(entity.text)\n",
    "    return pd.Series({'ORG': orgs, 'GPE': gpes, 'PERSON': people})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df[['Title']].copy()\n",
    "df_title = df_title.rename(columns={'Title':'Text'})\n",
    "df_abstract = df[['Abstract']].copy()\n",
    "df_abstract = df_abstract.rename(columns={'Abstract':'Text'})\n",
    "df_one = pd.concat([df_title, df_abstract], ignore_index=True)\n",
    "df_one['Text'] = df_one['Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = pd.concat([df_one[['Text']], df_one['Text'].apply(extract_entities)], axis=1)\n",
    "df_one = df_one.explode('GPE').reset_index(drop=True)\n",
    "df_one = df_one.explode('ORG').reset_index(drop=True)\n",
    "df_one = df_one.explode('PERSON').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_g = df_one.copy()\n",
    "df_one_g = df_one[['Text', 'GPE']]\n",
    "# df_one_g = df_one_g.fillna('')\n",
    "df_one_g = df_one_g.drop_duplicates(subset=['Text', 'GPE'])\n",
    "\n",
    "gpe_counts = df_one_g['GPE'].value_counts().reset_index()\n",
    "gpe_counts.columns = ['GPE', 'count']\n",
    "\n",
    "# pd.options.display.max_rows = None\n",
    "\n",
    "\n",
    "mapping_locations = {\n",
    "    'the United States': 'USA',\n",
    "    'The United States': 'USA',\n",
    "    'US': 'USA',\n",
    "    'U.S.': 'USA',\n",
    "    'United States' : 'USA',\n",
    "    'America' : 'USA',\n",
    "    'the United States of America' : 'USA',\n",
    "    'Britain' : 'UK',\n",
    "    'the United Kingdom': 'UK',\n",
    "    'U.K.' : 'UK',\n",
    "    'Global Britain' : 'UK',\n",
    "    'United Kingdom' : 'UK', \n",
    "    'the Soviet Union' : 'Russia',\n",
    "    'The Soviet Union' : 'Russia',\n",
    "    'USSR' : 'Russia',\n",
    "    'Ukraine - Perspective' : 'Ukraine',\n",
    "    'Ukrainian' : 'Ukraine',\n",
    "    'Great Britain' : 'UK',\n",
    "    'Ottoman Empire' : 'Turkey'\n",
    "}\n",
    "gpe_counts['GPE'] =gpe_counts['GPE'].replace(mapping_locations)\n",
    "gpe_counts = gpe_counts.groupby('GPE').sum().reset_index()\n",
    "gpe_counts.sort_values('count', ascending=False, inplace=True)\n",
    "gpe_counts = gpe_counts.reset_index(drop=True)\n",
    "gpe_counts.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_p = df_one.copy()\n",
    "df_one_p = df_one[['Text', 'PERSON']]\n",
    "# df_one_p = df_one_g.fillna('')\n",
    "df_one_p = df_one_p.drop_duplicates(subset=['Text', 'PERSON'])\n",
    "\n",
    "person_counts = df_one_p['PERSON'].value_counts().reset_index()\n",
    "person_counts.columns = ['PERSON', 'count']\n",
    "\n",
    "mapping_person = {\n",
    "    'Putin' : 'Vladimir Putin',\n",
    "    'Vladimir Putin' : 'Vladimir Putin',\n",
    "    'Churchill' : 'Winston Churchill',\n",
    "    'Hitler' : 'Adolf Hitler',\n",
    "    'Biden' : 'Joe Biden',\n",
    "    \"John le Carré’s\" : \"John le Carré\"\n",
    "}\n",
    "\n",
    "person_counts['PERSON'] =person_counts['PERSON'].replace(mapping_person)\n",
    "person_counts = person_counts.groupby('PERSON').sum().reset_index()\n",
    "person_counts.sort_values('count', ascending=False, inplace=True)\n",
    "\n",
    "remove_person = ['MI6', 'Twitter', 'GRU'\n",
    "          ]\n",
    "person_counts = person_counts[~person_counts['PERSON'].isin(remove_person)]\n",
    "person_counts = person_counts.reset_index(drop=True)\n",
    "\n",
    "person_counts.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_o = df_one.copy()\n",
    "df_one_o = df_one[['Text', 'ORG']]\n",
    "# df_one_p = df_one_g.fillna('')\n",
    "df_one_o = df_one_o.drop_duplicates(subset=['Text', 'ORG'])\n",
    "\n",
    "org_counts = df_one_o['ORG'].value_counts().reset_index()\n",
    "org_counts.columns = ['ORG', 'count']\n",
    "\n",
    "mapping_organisations = {\n",
    "    'The British Secret Intelligence Service' : 'SIS',\n",
    "    'the British Secret Intelligence Service' : 'SIS',\n",
    "    'The Joint Intelligence Committee' : 'Joint Intelligence Committee',\n",
    "    'the Joint Intelligence Committee' : 'Joint Intelligence Committee',\n",
    "    'Joint Intelligence Committee' : 'Joint Intelligence Committee',\n",
    "    'the Joint Intelligence Committee - History' : 'Joint Intelligence Committee',\n",
    "    'Central Intelligence Agency' : 'CIA',\n",
    "    'the Central Intelligence Agency' : 'CIA',\n",
    "    'the Foreign Office' : 'Foreign Office',\n",
    "    'Schar School' : 'Schar School of Policy and Government',\n",
    "    'the Secret Intelligence Service' : 'SIS',\n",
    "    \"George Mason University's\" : \"George Mason University\",\n",
    "    'JIC' : 'Joint Intelligence Committee'\n",
    "}\n",
    "org_counts['ORG'] =org_counts['ORG'].replace(mapping_organisations)\n",
    "org_counts = org_counts.groupby('ORG').sum().reset_index()\n",
    "org_counts.sort_values('count', ascending=False, inplace=True)\n",
    "\n",
    "remove_orgs = ['Intelligence', 'Kremlin', 'Ultra', 'International Security', 'Intelligence Studies', 'Intelligence Analysis', 'OSINT']\n",
    "org_counts = org_counts[~org_counts['ORG'].isin(remove_orgs)]\n",
    "org_counts = org_counts.reset_index(drop=True)\n",
    "\n",
    "org_counts.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe_counts.head(15).to_csv('gpe.csv')\n",
    "person_counts.head(15).to_csv('person.csv')\n",
    "org_counts.head(15).to_csv('org.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('all_items.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving citation count and open access status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doi = pd.read_csv('all_items.csv')\n",
    "\n",
    "df_doi = df_doi[['Zotero link', 'DOI']].dropna()\n",
    "df_doi = df_doi.drop(df_doi[df_doi['DOI'] == ''].index)\n",
    "df_doi = df_doi.reset_index(drop=True)\n",
    "df_doi['DOI'] = df_doi['DOI'].str.replace('https://doi.org/', '')\n",
    "\n",
    "def fetch_article_metadata(doi):\n",
    "    base_url = 'https://api.openalex.org/works/https://doi.org/'\n",
    "    response = requests.get(base_url + doi)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        counts_by_year = data.get('counts_by_year', [])\n",
    "        if counts_by_year:\n",
    "            first_citation_year = min(entry.get('year') for entry in data['counts_by_year'])\n",
    "        else:\n",
    "            first_citation_year = None\n",
    "        if data.get('counts_by_year'):\n",
    "            last_citation_year = max(entry.get('year') for entry in data['counts_by_year'])\n",
    "        else:\n",
    "            last_citation_year = None\n",
    "\n",
    "        article_metadata = {\n",
    "            'ID': data.get('id'),\n",
    "            'Citation': data.get('cited_by_count'),\n",
    "            'OA status': data.get('open_access', {}).get('is_oa'),\n",
    "            'Citation_list': data.get('cited_by_api_url'),\n",
    "            'First_citation_year': first_citation_year,\n",
    "            'Last_citation_year': last_citation_year,\n",
    "            'Publication_year': data.get('publication_year'),\n",
    "            'OA_link': data.get('open_access', {}).get('oa_url')\n",
    "        }\n",
    "        return article_metadata\n",
    "    else:\n",
    "        return {\n",
    "            'ID': None,\n",
    "            'Citation': None,\n",
    "            'OA status': None,\n",
    "            'First_citation_year': None,\n",
    "            'Last_citation_year': None,\n",
    "            'Publication_year': None,\n",
    "            'OA_link': None\n",
    "        }\n",
    "\n",
    "df_doi['ID'] = None\n",
    "df_doi['Citation'] = None\n",
    "df_doi['OA status'] = None\n",
    "df_doi['Citation_list'] = None\n",
    "df_doi['First_citation_year'] = None\n",
    "df_doi['Last_citation_year'] = None\n",
    "df_doi['Publication_year'] = None\n",
    "df_doi['OA_link'] = None\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_doi.iterrows():\n",
    "    doi = row['DOI']\n",
    "    article_metadata = fetch_article_metadata(doi)\n",
    "    if article_metadata:\n",
    "        # Update DataFrame with fetched information\n",
    "        df_doi.at[index, 'ID'] = article_metadata['ID']\n",
    "        df_doi.at[index, 'Citation'] = article_metadata['Citation']\n",
    "        df_doi.at[index, 'OA status'] = article_metadata['OA status']\n",
    "        df_doi.at[index, 'First_citation_year'] = article_metadata['First_citation_year']\n",
    "        df_doi.at[index, 'Last_citation_year'] = article_metadata['Last_citation_year']\n",
    "        df_doi.at[index, 'Citation_list'] = article_metadata.get('Citation_list', None)\n",
    "        df_doi.at[index, 'Publication_year'] = article_metadata['Publication_year']\n",
    "        df_doi.at[index, 'OA_link'] = article_metadata.get('OA_link', None)\n",
    "\n",
    "# Calculate the difference between First_citation_year and Publication_year\n",
    "df_doi['Year_difference'] = df_doi['First_citation_year'] - df_doi['Publication_year']\n",
    "df_doi.to_csv('citations.csv')\n",
    "\n",
    "df_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Remove after the first successful retrieval 2024-02-18\n",
    "\n",
    "# df_doi = pd.read_csv('all_items.csv')\n",
    "# if datetime.today().weekday() == 3:\n",
    "\n",
    "#     df_doi = df_doi[['Zotero link', 'DOI']].dropna()\n",
    "#     df_doi = df_doi.drop(df_doi[df_doi['DOI'] == ''].index)\n",
    "#     df_doi = df_doi.reset_index(drop=True)\n",
    "#     df_doi['DOI'] = df_doi['DOI'].str.replace('https://doi.org/', '')\n",
    "\n",
    "#     def fetch_article_metadata(doi):\n",
    "#         base_url = 'https://api.openalex.org/works/https://doi.org/'\n",
    "#         response = requests.get(base_url + doi)\n",
    "#         if response.status_code == 200:\n",
    "#             data = response.json()\n",
    "#             counts_by_year = data.get('counts_by_year', [])\n",
    "#             if counts_by_year:\n",
    "#                 first_citation_year = min(entry.get('year') for entry in data['counts_by_year'])\n",
    "#             else:\n",
    "#                 first_citation_year = None\n",
    "#             article_metadata = {\n",
    "#                 'ID': data.get('id'),\n",
    "#                 'Citation': data.get('cited_by_count'),\n",
    "#                 'OA status': data.get('open_access', {}).get('is_oa'),\n",
    "#                 'Citation_list': data.get('cited_by_api_url'),\n",
    "#                 'First_citation_year': first_citation_year,\n",
    "#                 'Publication_year':data.get('publication_year')\n",
    "#             }\n",
    "#             return article_metadata\n",
    "#         else:\n",
    "#             return {'ID': None, 'Citation': None, 'OA status': None, 'First_citation_year': None, 'Publication_year' : None}\n",
    "#     df_doi['ID'] = None\n",
    "#     df_doi['Citation'] = None\n",
    "#     df_doi['OA status'] = None\n",
    "#     df_doi['Citation_list'] = None\n",
    "#     df_doi['First_citation_year'] = None\n",
    "#     df_doi['Publication_year'] = None\n",
    "\n",
    "#     # Iterate over each row in the DataFrame\n",
    "#     for index, row in df_doi.iterrows():\n",
    "#         doi = row['DOI']\n",
    "#         article_metadata = fetch_article_metadata(doi)\n",
    "#         if article_metadata:\n",
    "#             # Update DataFrame with fetched information\n",
    "#             df_doi.at[index, 'ID'] = article_metadata['ID']\n",
    "#             df_doi.at[index, 'Citation'] = article_metadata['Citation']\n",
    "#             df_doi.at[index, 'OA status'] = article_metadata['OA status']\n",
    "#             df_doi.at[index, 'First_citation_year'] = article_metadata['First_citation_year']\n",
    "#             if 'Citation_list' in article_metadata:\n",
    "#                 df_doi.at[index, 'Citation_list'] = article_metadata['Citation_list']\n",
    "#             else:\n",
    "#                 # Set to None if key is not found\n",
    "#                 df_doi.at[index, 'Citation_list'] = None\n",
    "#             df_doi.at[index, 'Publication_year'] = article_metadata['Publication_year']\n",
    "\n",
    "#     # Calculate the difference between First_citation_year and Publication_year\n",
    "#     df_doi['Year_difference'] = df_doi['First_citation_year'] - df_doi['Publication_year']\n",
    "#     df_doi.to_csv('citations.csv')\n",
    "# else:\n",
    "#     print('This part will be refreshed every Wednesday!')\n",
    "# df_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Retrieving citation and open access status without first ciation year\n",
    "# df_doi = pd.read_csv('all_items.csv')\n",
    "# # df_doi = df_doi.head(10)\n",
    "# if datetime.today().weekday() == 5:\n",
    "\n",
    "#     df_doi = df_doi[['Zotero link', 'DOI']].dropna()\n",
    "#     df_doi = df_doi.drop(df_doi[df_doi['DOI']==''].index)\n",
    "#     df_doi = df_doi.reset_index(drop=True)\n",
    "#     df_doi['DOI'] = df_doi['DOI'].str.replace('https://doi.org/', '')\n",
    "#     df_doi\n",
    "\n",
    "#     def fetch_article_metadata(doi):\n",
    "#         base_url = 'https://api.openalex.org/works/https://doi.org/'\n",
    "#         response = requests.get(base_url + doi)\n",
    "#         if response.status_code == 200:\n",
    "#             data = response.json()\n",
    "#             article_metadata = {\n",
    "#                 'ID': data.get('id'),\n",
    "#                 'Citation': data.get('cited_by_count'),\n",
    "#                 'OA status': data.get('open_access', {}).get('is_oa'),\n",
    "#                 'Citation_list':data.get('cited_by_api_url')\n",
    "#             }\n",
    "#             return article_metadata\n",
    "#         else:\n",
    "#             return {'ID': None, 'Citation': None, 'OA status': None}\n",
    "\n",
    "#     df_doi['ID'] = None\n",
    "#     df_doi['Citation'] = None\n",
    "#     df_doi['OA status'] = None\n",
    "#     df_doi['Citation_list'] = None\n",
    "\n",
    "#     # Iterate over each row in the DataFrame\n",
    "#     for index, row in df_doi.iterrows():\n",
    "#         doi = row['DOI']\n",
    "#         article_metadata = fetch_article_metadata(doi)\n",
    "#         if article_metadata:\n",
    "#             # Update DataFrame with fetched information\n",
    "#             df_doi.at[index, 'ID'] = article_metadata['ID']\n",
    "#             df_doi.at[index, 'Citation'] = article_metadata['Citation']\n",
    "#             df_doi.at[index, 'OA status'] = article_metadata['OA status']\n",
    "#             if 'Citation_list' in article_metadata:\n",
    "#                 df_doi.at[index, 'Citation_list'] = article_metadata['Citation_list']\n",
    "#             else:\n",
    "#                 # Set to None if key is not found\n",
    "#                 df_doi.at[index, 'Citation_list'] = None\n",
    "#     df_doi\n",
    "#     df_doi.to_csv('citations.csv')\n",
    "# else:\n",
    "#     print('This part will be refreshed every Wednesday!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doi_2 = pd.read_csv('citations.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_doi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('all_items.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_doi_1 = pd.read_csv('citations.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_doi_1 = df_doi_1.drop(columns=['DOI'])\n",
    "df_1 = pd.merge(df_1, df_doi_1, on='Zotero link', how='left')\n",
    "df_1.to_csv('all_items.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('all_items_duplicated.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_doi_2 = pd.read_csv('citations.csv', usecols=lambda column: column != 'Unnamed: 0')\n",
    "df_doi_2 = df_doi_2.drop(columns=['DOI'])\n",
    "df_2 = pd.merge(df_2, df_doi_2, on='Zotero link', how='left')\n",
    "df_2.to_csv('all_items_duplicated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv('all_items_duplicated.csv')\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = df_2[df_2['DOI']=='10.1080/08850600600829882']\n",
    "ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_zotero_id = df_2[['Zotero link']].copy()\n",
    "# df_zotero_id['Zotero link'] = df_zotero_id['Zotero link'].str.replace(\n",
    "#     'https://www.zotero.org/groups/intelarchive_intelligence_studies_database/items/', ''\n",
    "# )\n",
    "# df_zotero_id = df_zotero_id.drop_duplicates(subset='Zotero link').reset_index(drop=True)\n",
    "# df_zotero_id = df_zotero_id.rename(columns={'Zotero link':'zotero_item_key'})\n",
    "# # df_zotero_id = df_zotero_id.head(100)\n",
    "# df_zotero_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zotero_id = pd.read_csv('zotero_citation_format.csv')\n",
    "df_all = pd.read_csv('all_items.csv')\n",
    "df_all = df_all[['Zotero link']]\n",
    "df_all['zotero_item_key'] = df_all['Zotero link'].str.replace('https://www.zotero.org/groups/intelarchive_intelligence_studies_database/items/', '')\n",
    "df_all = df_all.drop_duplicates()\n",
    "df_not_zotero_id = df_all[~df_all['zotero_item_key'].isin(df_zotero_id['zotero_item_key'])]\n",
    "df_not_zotero_id = df_not_zotero_id[['zotero_item_key']].reset_index(drop=True)\n",
    "df_not_zotero_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = '2514686'\n",
    "\n",
    "# Base URL for Zotero API\n",
    "base_url = 'https://api.zotero.org'\n",
    "\n",
    "# Initialize an empty string to accumulate bibliographies\n",
    "all_bibliographies = \"\"\n",
    "\n",
    "# List to store bibliographies\n",
    "bibliographies = []\n",
    "\n",
    "# Iterate through each item key in the DataFrame\n",
    "for item_key in df_not_zotero_id['zotero_item_key']:\n",
    "    # Endpoint to get item bibliography\n",
    "    endpoint = f'/groups/{user_id}/items/{item_key}'\n",
    "\n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'format': 'bib',\n",
    "        'linkwrap': 1\n",
    "    }\n",
    "\n",
    "    # Make GET request to Zotero API\n",
    "    response = requests.get(base_url + endpoint, params=params)\n",
    "\n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        bibliography = response.text.strip()  # Strip any leading/trailing whitespace\n",
    "        bibliographies.append(bibliography)\n",
    "        all_bibliographies += f'<p>{bibliography}</p><br><br>'  # Append bibliography with two newlines for separation\n",
    "    else:\n",
    "        error_message = f'Error fetching bibliography for item {item_key}: Status Code {response.status_code}'\n",
    "        bibliographies.append(error_message)\n",
    "        all_bibliographies += f'<p>{error_message}</p><br><br>'\n",
    "\n",
    "# Add bibliographies to the original DataFrame\n",
    "df_not_zotero_id['bibliography'] = bibliographies\n",
    "df_not_zotero_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_not_zotero_id['bibliography'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bibliographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zotero_id = pd.read_csv('zotero_citation_format.csv', index_col=False)\n",
    "df_zotero_id = df_zotero_id.drop(columns={'Unnamed: 0'})\n",
    "df_zotero_id = pd.concat([df_zotero_id, df_not_zotero_id]).reset_index(drop=True)\n",
    "df_zotero_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemL5K2YZ2M = df_zotero_id[df_zotero_id['zotero_item_key']=='JQ6BYCU9']\n",
    "itemL5K2YZ2M\n",
    "\n",
    "print(itemL5K2YZ2M['bibliography'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zotero_id.to_csv('zotero_citation_format.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad80e88e3d0f45253c4e09d27534efe22e9bb02b1c82d05b21d80e484bb90b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
